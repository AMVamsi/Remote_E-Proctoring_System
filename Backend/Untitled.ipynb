{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "\n",
    "def read_audio(stream, filename):\n",
    "    chunk = 1024  # Record in chunks of 1024 samples\n",
    "    sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "    channels = 1\n",
    "    fs = 44100  # Record at 44100 samples per second\n",
    "    seconds = 10 # Number of seconds to record at once\n",
    "    filename = filename\n",
    "    frames = []  # Initialize array to store frames\n",
    "    \n",
    "    for i in range(0, int(fs / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "    \n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    # Stop and close the stream\n",
    "    #stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "def convert(i):\n",
    "    if i >= 0:\n",
    "        sound = 'record' + str(i) +'.wav'\n",
    "        r = sr.Recognizer()\n",
    "        \n",
    "        with sr.AudioFile(sound) as source:\n",
    "            r.adjust_for_ambient_noise(source)\n",
    "            print(\"Converting Audio To Text and saving to file..... \") \n",
    "            audio = r.listen(source)\n",
    "        try:\n",
    "            value = r.recognize_google(audio) ##### API call to google for speech recognition\n",
    "            os.remove(sound)\n",
    "            if str is bytes: \n",
    "                result = u\"{}\".format(value).encode(\"utf-8\")\n",
    "            else: \n",
    "                result = \"{}\".format(value)\n",
    " \n",
    "            with open(\"test.txt\",\"a\") as f:\n",
    "                f.write(result)\n",
    "                f.write(\" \")\n",
    "                f.close()\n",
    "                \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"{0}\".format(e))\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "chunk = 1024  # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 1\n",
    "fs = 44100\n",
    "\n",
    "def save_audios(i):\n",
    "    stream = p.open(format=sample_format,channels=channels,rate=fs,\n",
    "                frames_per_buffer=chunk,input=True)\n",
    "    filename = 'record'+str(i)+'.wav'\n",
    "    read_audio(stream, filename)\n",
    "\n",
    "\n",
    "'''for i in range(30//10): # Number of total seconds to record/ Number of seconds per recording\n",
    "    t1 = threading.Thread(target=save_audios, args=[i]) \n",
    "    x = i-1\n",
    "    t2 = threading.Thread(target=convert, args=[x]) # send one earlier than being recorded\n",
    "    t1.start() \n",
    "    t1.join() \n",
    "    t2.start() \n",
    "    t2.join()\n",
    "    save_audios(i)\n",
    "    convert(x)\n",
    "    if i==2:\n",
    "        flag = True\n",
    "if flag:\n",
    "    convert(i)\n",
    "    p.terminate()'''\n",
    "for i in range(30//10): \n",
    "\tsave_audios(i)\n",
    "\tx = i-1\n",
    "\tconvert(x) # send one earlier than being recorded\n",
    "\n",
    "\tif i==2:\n",
    "\t\tflag = True\n",
    "if flag:\n",
    "    convert(i)\n",
    "    #p.terminate()\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "file = open(\"test.txt\") ## Student speech file\n",
    "data = file.read()\n",
    "file.close()\n",
    "stop_words = set(stopwords.words('english'))   \n",
    "word_tokens = word_tokenize(data) ######### tokenizing sentence\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens:   ####### Removing stop words\n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "####### creating a final file\n",
    "f=open('final.txt','w')\n",
    "for ele in filtered_sentence:\n",
    "    f.write(ele+' ')\n",
    "f.close()\n",
    "    \n",
    "##### checking whether proctor needs to be alerted or not\n",
    "file = open(\"paper.txt\") ## Question file\n",
    "data = file.read()\n",
    "file.close()\n",
    "stop_words = set(stopwords.words('english'))   \n",
    "word_tokens = word_tokenize(data) ######### tokenizing sentence\n",
    "filtered_questions = [w for w in word_tokens if not w in stop_words]  \n",
    "filtered_questions = [] \n",
    "  \n",
    "for w in word_tokens:   ####### Removing stop words\n",
    "    if w not in stop_words: \n",
    "        filtered_questions.append(w) \n",
    "        \n",
    "def common_member(a, b):     \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "      \n",
    "    # check length  \n",
    "    if len(a_set.intersection(b_set)) > 0: \n",
    "        return(a_set.intersection(b_set))   \n",
    "    else: \n",
    "        return([]) \n",
    "\n",
    "comm = common_member(filtered_questions, filtered_sentence)\n",
    "print('Number of common elements:', len(comm))\n",
    "print(comm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/msml/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/msml/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-17aa64cb8664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
